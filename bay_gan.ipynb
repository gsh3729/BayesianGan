{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import os\nimport sys\nfrom tqdm import tqdm\n\nfrom matplotlib import pyplot as plt ##matplotlib.use('Agg') should call before pyplot for file printing\n\nimport tensorflow as tf\nimport numpy as np\n\nfrom sklearn import mixture\nimport math\nfrom tensorflow.python.framework import ops\n\nimport glob\nimport six\n# import cPickle\nfrom scipy.ndimage import imread\nfrom scipy.misc import imresize\nimport scipy.io as sio\nfrom collections import OrderedDict, defaultdict\n\nfrom scipy import stats\n\nfrom sklearn.decomposition import PCA","execution_count":1,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"class SynthDataset():   \n    def __init__(self, x_dim=100, z_dim=2, num_clusters=10, seed=1234):       \n        np.random.seed(seed)\n        \n        self.x_dim = x_dim\n        self.N = 10000\n        self.true_z_dim = z_dim\n        \n        # generate synthetic data\n        self.Xs = []\n        for _ in range(0,num_clusters):\n            cluster_mean = np.random.randn(self.true_z_dim) * 5 # to make them more spread\n            A = np.random.randn(self.x_dim, self.true_z_dim) * 5\n            X = np.dot(np.random.randn(int(self.N / num_clusters), self.true_z_dim) + cluster_mean, A.T) ##matrix mul\n            self.Xs.append(X)\n        X_raw = np.concatenate(self.Xs)\n        self.X = (X_raw - X_raw.mean(0)) / (X_raw.std(0))\n        print (\"Dataset shape\", self.X.shape)\n        \n        \n    def next_batch(self, batch_size):\n        rand_idx = np.random.choice(range(self.N), size=(batch_size,), replace=False)\n        return self.X[rand_idx]\n    \n    def visualize_data(self):\n        pca = PCA(n_components=2)\n        data = pca.fit_transform(self.X)\n        plt.scatter(data[:,0],data[:,1])\n        return data","execution_count":2,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"a = SynthDataset()\ndata = a.visualize_data()","execution_count":3,"outputs":[{"output_type":"stream","text":"Dataset shape (10000, 100)\n","name":"stdout"},{"output_type":"display_data","data":{"text/plain":"<Figure size 432x288 with 1 Axes>","image/png":"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\n"},"metadata":{"needs_background":"light"}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def gmm_ms(X):\n    aics = []\n    n_components_range = range(1, 20)\n    for n_components in n_components_range:\n        # Fit a Gaussian mixture with EM\n        gmm = mixture.GaussianMixture(n_components=n_components,covariance_type=\"full\")\n        gmm.fit(X)\n        aics.append(gmm.aic(X))\n    return np.array(aics)\n\ndef kl_div(p, q):\n        eps = 1e-10\n        p_safe = np.copy(p)\n        p_safe[p_safe < eps] = eps\n        q_safe = np.copy(q)\n        q_safe[q_safe < eps] = eps\n        return np.sum(p_safe * (np.log(p_safe) - np.log(q_safe)))\n    \ndef js_div(p, q):\n        m = (p + q) / 2.\n        return (kl_div(p, m) + kl_div(q, m))/2.\n    \ndef analyze_div(X_real, X_sample):\n    pca = PCA(n_components=2)\n    X_trans_real = pca.fit_transform(X_real)\n    X_trans_fake = pca.transform(X_sample)\n    \n    dx = 0.1\n    dy = 0.1\n    \n    xmin1 = np.min(X_trans_real[:, 0]) - 3.0\n    xmax1 = np.max(X_trans_real[:, 0]) + 3.0\n    \n    xmin2 = np.min(X_trans_real[:, 1]) - 3.0\n    xmax2 = np.max(X_trans_real[:, 1]) + 3.0\n    \n    space = np.dstack(np.meshgrid(np.arange(xmin1,xmax1,dx), np.arange(xmin2,xmax2,dy))).reshape(-1, 2).T\n\n    real_kde = stats.gaussian_kde(X_trans_real.T)\n    real_density = real_kde(space) * dx * dy\n\n    fake_kde = stats.gaussian_kde(X_trans_fake.T)\n    fake_density = fake_kde(space) * dx * dy\n    \n    return js_div(real_density, fake_density), X_trans_real, X_trans_fake","execution_count":4,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class BGAN(object):\n    def __init__(self, x_dim, z_dim, dataset_size, batch_size=64, prior_std=1.0, J=1, M=50, num_classes=1, alpha=0.01, lr=0.0002,\n                 optimizer='adam', ml=False):\n        self.batch_size = batch_size\n        self.dataset_size = dataset_size\n        self.x_dim = x_dim\n        self.z_dim = z_dim\n        self.optimizer = optimizer\n        self.prior_std = prior_std\n        self.num_gen = J\n        self.num_mcmc = M\n        self.alpha = alpha\n        self.lr = lr\n        self.ml = ml\n        if self.ml:\n            assert self.num_gen*self.num_mcmc == 1, 'cannot have multiple generators in ml mode'\n        self.weight_dims = OrderedDict([('g_h0_lin_w', (self.z_dim, 1000)),('g_h0_lin_b', (1000, )), ('g_lin_w',(1000, self.x_dim)),('g_lin_b',(self.x_dim,))]) ##changed the final x dimension\n        self.sghmc_noise = {}\n        self.noise_std = np.sqrt(2*self.alpha)\n        for name, dim in self.weight_dims.items():\n#             self.sghmc_noise[name] = tf.contrib.distributions.Normal(mu=0. , sigma=self.noise_std*tf.ones(self.weight_dims[name])) ##check normal distributions\n            self.sghmc_noise[name] = tf.contrib.distributions.Normal(loc=0. , scale=self.noise_std*tf.ones(self.weight_dims[name])) ##check normal distributions\n\n        self.K = num_classes\n        self.build_bgan_graph()\n        \n    def linear(self, input_, output_size, scope=None, matrix=None, bias=None):\n        shape = input_.get_shape().as_list()\n        with tf.variable_scope(scope or 'Linear'):\n            if matrix is None:\n                matrix = tf.get_variable(\"Matrix\", [shape[1], output_size], dtype=tf.float32, initializer=tf.random_normal_initializer(stddev=0.02))\n            if bias is None:\n                bias = tf.get_variable(\"bias\", [output_size], initializer = tf.constant_initializer(0.0))\n            return tf.matmul(input_, matrix) + bias\n        \n    def generator(self, z, gen_params):\n        with tf.variable_scope('generator') as scope:\n            h = self.linear(z, 1000, 'g_h0_lin', gen_params['g_h0_lin_w'], gen_params['g_h0_lin_b']) ##see when is matrix in linear is created i.e matrix=None\n            h0 = tf.maximum(h, 0.2*h)\n            self.x_ = self.linear(h0, self.x_dim, 'g_lin', gen_params['g_lin_w'], gen_params['g_lin_b'])\n            return self.x_ ##check why its self.x_\n        \n    def discriminator(self, x, K, reuse=False):\n        with tf.variable_scope('discriminator') as scope:\n            if reuse:\n                scope.reuse_variables()\n            h = self.linear(x, 1000, 'd_lin_0')\n            h0 = tf.maximum(h, 0.2*h)\n            h1 = self.linear(h0, K, 'd_lin_1')\n            return tf.nn.softmax(h1), h1 ##check the softmax layer\n        \n    def sampler(self, z, gen_params): ##why cant use directly\n        with tf.variable_scope('generator') as scope:\n            scope.reuse_variables()\n            return self.generator(z, gen_params)\n    \n    def gen_prior(self, gen_params): ##why do we want scope here as generator from where dict is coming\n        with tf.variable_scope('generator') as scope:\n            prior_loss = 0.0\n            for var in gen_params.values():\n                nn = tf.divide(var, self.prior_std)\n                prior_loss += tf.reduce_mean(tf.multiply(nn, nn))\n        prior_loss /= self.dataset_size\n        return prior_loss\n    \n    def gen_noise(self, gen_params): #sample ?? \n        with tf.variable_scope('generator') as scope:\n            noise_loss = 0.0\n            for name, var in gen_params.items():\n                noise_loss += tf.reduce_sum(var * self.sghmc_noise[name].sample())\n        noise_loss /= self.dataset_size\n        return noise_loss\n    \n    def disc_prior(self):## why using self.d_vars\n        with tf.variable_scope('discriminator') as scope:\n            prior_loss = 0.0\n            for var in self.d_vars:\n                nn = tf.divide(var, self.prior_std)\n                prior_loss += tf.reduce_mean(tf.multiply(nn, nn))\n        prior_loss /= self.dataset_size\n        return prior_loss\n    \n    def disc_noise(self): ## sample ? \n        with tf.variable_scope('discriminator') as scope:\n            noise_loss = 0.0\n            for var in self.d_vars:\n#                 noise_ = tf.contrib.distributions.Normal(mu=0.0, sigma=self.noise_std*tf.ones(var.get_shape()))\n                noise_ = tf.contrib.distributions.Normal(loc=0.0, scale=self.noise_std*tf.ones(var.get_shape()))\n                noise_loss += tf.reduce_sum(var * noise_.sample())\n        noise_loss /= self.dataset_size\n        return noise_loss\n\n    \n    def build_bgan_graph(self):\n        self.inputs = tf.placeholder(tf.float32, shape=(self.batch_size,self.x_dim), name = 'real_images') ##check why addition of x_dim\n        self.labels = tf.placeholder(tf.float32, shape=(self.batch_size,self.K+1), name='real_targets')\n        self.z = tf.placeholder(tf.float32, shape=(None, self.z_dim), name='z')\n        \n        self.gen_param_list = []\n        with tf.variable_scope('generator') as scope:\n            for g in range(self.num_gen):\n                for m in range(self.num_mcmc):\n                    gen_params = {}  ##check with attributedict method in code\n                    for name, shape in self.weight_dims.items():\n                        gen_params[name] = tf.get_variable(\"%s_%04d_%04d\" % (name, g, m), shape, initializer=tf.random_normal_initializer(stddev=0.02))\n                    self.gen_param_list.append(gen_params)\n                    \n        self.D, self.D_logits = self.discriminator(self.inputs, self.K+1) ##loading real data\n        \n        constant_labels = np.zeros((self.batch_size, 2)) ##why labels are two size\n        constant_labels[:, 1] = 1.0\n        self.d_loss_real = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=self.D_logits,labels=tf.constant(constant_labels))) ##check loss function\n        \n        self.generation = defaultdict(list)\n        for gen_params in self.gen_param_list:\n            self.generation['g_prior'].append(self.gen_prior(gen_params))\n            self.generation['g_noise'].append(self.gen_noise(gen_params))\n            self.generation['generators'].append(self.generator(self.z, gen_params))\n            self.generation['gen_samplers'].append(self.sampler(self.z, gen_params))\n            D_, D_logits = self.discriminator(self.generator(self.z, gen_params), self.K+1, reuse=True) ##why calling generator again\n            self.generation['d_logits'].append(D_logits)\n            self.generation['d_probs'].append(D_)\n            \n        d_loss_fakes = []\n        constant_labels = np.zeros((self.batch_size, self.K+1))\n        constant_labels[:,0] = 1.0\n        for logits in self.generation['d_logits']:\n            d_loss_fakes.append(tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits,labels=tf.constant(constant_labels)))) ##check why it is not stored in self as like self.d_loss_real\n            \n        t_vars = tf.trainable_variables()\n        self.d_vars = [var for var in t_vars if 'd_' in var.name] ##check this d_\n        \n        d_losses = []\n        for d_loss_fake_ in d_loss_fakes:\n            d_loss_ = self.d_loss_real + d_loss_fake_\n            if not self.ml: ##why is this ml for\n                d_loss_ += self.disc_prior()+self.disc_noise()\n            d_losses.append(tf.reshape(d_loss_, [1])) ##check reshpae\n        self.d_loss = tf.reduce_logsumexp(tf.concat(d_losses,0)) ## why logsumexp and concat ##1.4version axis=0 no concat\n        \n        self.g_vars = []\n        for g in range(self.num_gen):\n            for m in range(self.num_mcmc):\n                self.g_vars.append([var for var in t_vars if 'g_' in var.name and \"_%04d_%04d\" % (g, m) in var.name])\n        \n        \n        self.d_learning_rate = tf.placeholder(tf.float32, shape=[])\n        d_opt = tf.train.AdamOptimizer(learning_rate=self.d_learning_rate, beta1=0.5)\n        self.d_optim = d_opt.minimize(self.d_loss, var_list=self.d_vars) ##skipped d_optim_adam in code\n        \n        self.g_optims = []\n        self.g_learning_rate = tf.placeholder(tf.float32, shape=[])\n        for g in range(self.num_gen * self.num_mcmc):\n            g_loss = -tf.reduce_mean(tf.log((1.0 - self.generation['d_probs'][g][:,0]) + 1e-8))\n            if not self.ml:\n                g_loss += self.generation['g_prior'][g] + self.generation['g_noise'][g]\n            self.generation['g_losses'].append(g_loss)\n            g_opt = tf.train.AdamOptimizer(learning_rate=self.g_learning_rate, beta1=0.5)\n            self.g_optims.append(g_opt.minimize(g_loss, var_list = self.g_vars[g]))","execution_count":5,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_session(): ##understand sessions in tf\n    global _SESSION\n    if tf.get_default_session() is None:  \n        _SESSION = tf.InteractiveSession()\n    else:\n        _SESSION = tf.get_default_session()\n\n    return _SESSION","execution_count":6,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"batch_size = 64\niterations = 240\nz_dim = 10\nlr_decay = 3.\nbase_lr = 1e-2\nnum_generators = 5\nmcmc_samples = 100\nresults_path = './results/'\nif not os.path.exists(results_path):\n    os.makedirs(results_path)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"syn_data = SynthDataset()\naics = gmm_ms(syn_data.X)    \nprint(\"Number of clusters in the data (AIC Estimate):\", aics.argmin())\nbgan = BGAN(x_dim=syn_data.x_dim, z_dim=z_dim, dataset_size=syn_data.N, J=num_generators, M=mcmc_samples)\nprint(\"Starting session\")\n# tf.reset_default_graph()\nsession = get_session()\ntf.global_variables_initializer().run()\nprint(\"Starting training loop\")\nsample_z = np.random.uniform(-1, 1, size=(batch_size, z_dim))\nall_data_fake = []\nall_aics_fake = []\nall_div = []\ndis_loss = []\ngen_loss = defaultdict(list)","execution_count":null,"outputs":[{"output_type":"stream","text":"Dataset shape (10000, 100)\nNumber of clusters in the data (AIC Estimate): 10\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Iteration starts\")\n# for iteration in tqdm(range(0,iterations)):\nfor iteration in range(0,iterations):\n    learning_rate = base_lr * np.exp(-lr_decay * min(1.0, (iteration*batch_size)/float(syn_data.N)))\n    batch_z = np.random.uniform(-1, 1, [batch_size, z_dim])\n    input_batch = syn_data.next_batch(batch_size)\n    _, d_loss = session.run([bgan.d_optim, bgan.d_loss], feed_dict={bgan.inputs: input_batch, bgan.z: batch_z, bgan.d_learning_rate: learning_rate})\n\n    g_losses = []\n    for g in range(0, bgan.num_gen): # compute g_sample loss\n        batch_z = np.random.uniform(-1, 1, [batch_size, z_dim]) ## redundant\n        _, g_loss = session.run([bgan.g_optims[g], bgan.generation[\"g_losses\"][g]], feed_dict={bgan.z: batch_z, bgan.g_learning_rate: learning_rate})\n        g_losses.append(g_loss)\n        \n#     print(\"Disc loss = %.2f\" % d_loss, \", Gen loss = \", \", \".join([\"%.2f\" % gl for gl in g_losses])) \n    dis_loss.append(d_loss)\n    for i,loss in enumerate(g_losses, start=1):\n        gen_loss[i].append(loss)\n\n    if (iteration + 1) % 10 == 0:\n        labels = []\n        plt.figure(figsize=(5,3))\n        plt.title('Loss_curve')\n        plt.xlabel('Iterations')\n        plt.plot(dis_loss)\n        labels.append('Dis-loss')\n        for key,value in gen_loss.items():\n            plt.plot(value)\n            labels.append('Gen'+str(key)+'-loss')\n        plt.legend(labels, loc='upper left')\n        plt.show()\n        \n        print(\"Running GMM on sample data\")\n        fake_data = [] ##generating data from the distribution\n        for num_samples in range(0, 10):\n            for g in range(0, bgan.num_gen):\n                # collect sample\n                sample_z = np.random.uniform(-1, 1, size=(batch_size, z_dim)) ## redundant\n                sampled_data = session.run(bgan.generation[\"gen_samplers\"][g], feed_dict={bgan.z: sample_z})\n                fake_data.append(sampled_data)\n                \n        X_real = syn_data.X\n        X_sample = np.concatenate(fake_data) ##why np.concatenate\n        all_data_fake.append(X_sample)\n        \n        aics_fake = gmm_ms(X_sample)\n        all_aics_fake.append(aics_fake)\n        print(\"Fake number of clusters (AIC Estimate):\", aics_fake.argmin())\n        \n        div, X_trans_real, X_trans_fake = analyze_div(X_real, X_sample)\n        all_div.append(div)\n        print(\"JS div\", div) \n        \n        plt.figure(figsize=(5,3))\n        plt.title('JS Div')\n        plt.xlabel('Iterations')\n        plt.plot(all_div)\n        plt.show()\n        \n        fig, ax_arr = plt.subplots(1,2)\n        xmin1 = np.min(X_trans_real[:, 0]) - 1.0\n        xmax1 = np.max(X_trans_real[:, 0]) + 1.0\n        xmin2 = np.min(X_trans_real[:, 1]) - 1.0\n        xmax2 = np.max(X_trans_real[:, 1]) + 1.0\n        ax_arr[0].plot(X_trans_real[:, 0], X_trans_real[:, 1], '.r')\n        ax_arr[0].set_xlim([xmin1, xmax1]); ax_arr[0].set_ylim([xmin2, xmax2])\n        ax_arr[1].plot(X_trans_fake[:, 0], X_trans_fake[:, 1], '.g')\n        ax_arr[1].set_xlim([xmin1, xmax1]); ax_arr[1].set_ylim([xmin2, xmax2])\n        ax_arr[0].set_aspect('equal', adjustable='box')\n        ax_arr[1].set_aspect('equal', adjustable='box')\n        ax_arr[0].set_title(\"Iteration %i\" % (iteration+1))\n        ax_arr[1].set_title(\"Iteration %i\" % (iteration+1))\n        plt.show()\n        save_weights=True\n        if save_weights:\n            var_dict = {}\n            for var in tf.trainable_variables():\n                var_dict[var.name] = session.run(var.name)\n            np.savez_compressed(os.path.join(results_path, \"weights_%i.npz\" % iteration),**var_dict)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# cluster_mean = np.random.randn(2) * 5 # to make them more spread\n# print(cluster_mean)\n# A = np.random.randn(100, 2) * 5\n# print(A.shape)\n# b = A + cluster_mean\n# print(b.shape)\n# X = np.dot(np.random.randn(1000, 2) + cluster_mean, A.T)##1000 points in cluster\n# print(X.shape)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}